{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'third-party'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package dolch to /home/kunal/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection third-party\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('third-party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /home/kunal/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('state_union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package udhr to /home/kunal/nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('udhr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello students, how are you doing today?', 'The olympics are inspiring, and Python is awesome.', 'You look nice today.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Hello students, how are you doing today? The olympics are inspiring, and Python is awesome. You look nice today.\"\n",
    "\n",
    "print(sent_tokenize(text))#sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'students', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'olympics', 'are', 'inspiring', ',', 'and', 'Python', 'is', 'awesome', '.', 'You', 'look', 'nice', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))#word tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'your', 'i', 'same', 'should', 'over', \"don't\", 'whom', 'from', 'won', 'too', 'have', \"aren't\", 'did', 'its', \"she's\", 'why', 'is', 'of', 'at', 'such', 'y', 'ain', \"you're\", 'down', 'wouldn', 'theirs', 'he', 'or', 'this', \"you've\", 'my', 'do', 'you', 'it', 'him', 'didn', 'o', 'isn', 'will', \"wouldn't\", 'when', 'doesn', 'and', 'in', 'who', \"couldn't\", 'while', \"wasn't\", 'does', \"doesn't\", \"isn't\", 'a', \"mightn't\", 'hadn', 'than', 'each', 'herself', 'these', 'an', 'most', 'own', 'am', 'on', \"you'll\", 'again', 'so', 'any', 'yours', 're', 'mustn', 'that', 'with', 'myself', 'hers', 'll', \"hasn't\", \"won't\", 'just', 's', 'don', 'then', 'shouldn', 'were', 'our', 'above', 'about', \"you'd\", 'me', 'we', 'had', 'to', 'but', 'd', 'into', 'only', 'now', 'if', 'both', 'nor', 'for', \"that'll\", 'not', 'off', 'be', \"didn't\", 'before', 'their', 'themselves', 'here', 'ours', 'weren', 'them', 'the', 'wasn', 'shan', 'how', 'his', 'aren', 'being', 'few', \"mustn't\", 'between', 'yourselves', \"haven't\", 'has', 'below', 'yourself', 'against', 'itself', 'ma', 'been', 'all', \"hadn't\", 'no', 'up', 'out', 'where', 't', 'couldn', 'until', 'mightn', 'through', 've', 'she', 'because', 'hasn', 'having', 'very', 'after', 'what', 'some', 'was', 'her', 'doing', 'more', 'm', 'during', 'himself', 'under', 'which', 'as', 'are', \"should've\", \"shan't\", \"weren't\", 'can', \"it's\", 'ourselves', 'once', 'those', 'by', 'needn', 'further', 'there', 'they', 'haven', \"shouldn't\", \"needn't\", 'other'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'some', 'sample', 'text', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'text', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is some sample text, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "word_tokens=word_tokenize(example_sent)\n",
    "\n",
    "#remve stop words\n",
    "filtered_sent=[w for w in word_tokens if w not in stop_words]\n",
    "print(word_tokens)\n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Words with NLTK:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride\n",
      "ride\n",
      "rider\n",
      "ride\n",
      "rode\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "example_words = [\"ride\",\"riding\",\"rider\",\"rides\",\"rode\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when\n",
      "rider\n",
      "are\n",
      "ride\n",
      "their\n",
      "hors\n",
      ",\n",
      "they\n",
      "often\n",
      "think\n",
      "of\n",
      "how\n",
      "cowboy\n",
      "rode\n",
      "hors\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "new_text = \"When riders are riding their horses, they often think of how cowboys rode horses.\"\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kunal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Declaration of Human Rights\n",
      "Preamble\n",
      "Whereas recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world, \n",
      "\n",
      "Whereas disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind, and the advent of a world in which human beings shall enjoy freedom of speech and belief and freedom from fear and want has been proclaimed as the highest aspiration of the common people, \n",
      "\n",
      "Whereas it is essential, if man is not to be compelled to have recourse, as a last resort, to rebellion against tyranny and oppression, that human rights should be protected by the rule of law, \n",
      "\n",
      "Whereas it is essential to promote the development of friendly relations between nations, \n",
      "\n",
      "Whereas the peoples of the United Nations have in the Charter reaffirmed their faith in fundamental human rights, in the dignity and worth of the human person and in\n"
     ]
    }
   ],
   "source": [
    "#As an example, lets load the universal declaration of human rights.\n",
    "from nltk.corpus import udhr\n",
    "\n",
    "#prints only firts 1000 characters\n",
    "print(udhr.raw('English-Latin1')[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "February 2, 2005\n",
      "\n",
      "\n",
      "9:10 P.M. EST \n",
      "\n",
      "THE PRESIDENT: Mr. Speaker, Vice President Cheney, members of Congress, fellow citizens: \n",
      "\n",
      "As a new Congress gathers, all of us in the elected branches of government share a great privilege: We've been placed in office by the votes of the people we serve. And tonight that is a privilege we share with newly-elected leaders of Afghanistan, the Palestinian Territories, Ukraine, and a free and sovereign Iraq. (Applause.) \n",
      "\n",
      "Two weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all. This evening I will set forth policies to advance that ideal at home and around the world. \n",
      "\n",
      "Tonight, with a healthy, growing economy, with more Americans going back to work, with our nation an active force for good in the world -- the state of our union is confident and strong. (Applause.) \n",
      "\n",
      "Our generati\n",
      "======\n",
      "PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "January 31, 2006\n",
      "\n",
      "THE PRESIDENT: Thank you all. Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream. Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King. (Applause.)\n",
      "\n",
      "President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan. 31, 2006. White House photo by Eric DraperEvery time I'm invited to this rostrum, I'm humbled by the privilege, and mindful of the history we've seen together. We have gathered under this Capitol dome in moments of national mourning and national achievement. We have served America \n"
     ]
    }
   ],
   "source": [
    "# Lets import some sample and training text - George Bush's 2005 and 2006 state of the union addresses. \n",
    "\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "#both speeches of george bush have similar language;tokenizer will work much better than a generic tokenizer since we train it on his speech\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "print(train_text[:1000])\n",
    "print(\"======\")\n",
    "print(sample_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the PunktSentenceTokenizer\n",
    "custom_sentence_tokenizer=PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\", 'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.', 'Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King.', '(Applause.)', 'President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize(sentence) the sample_text using our trained tokenizer (cuz the dataset has content like (Applause))\n",
    "tokenized=custom_sentence_tokenizer.tokenize(sample_text)\n",
    "print(tokenized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# This function will tag each tokenized word with a part of speech\n",
    "def tokword_to_partofspeech():\n",
    "    try:\n",
    "        for i in tokenized[:5]:#tagging only first 5 sentences\n",
    "            words=nltk.word_tokenize(i)\n",
    "            tagged=nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "#The output is a list of tuples - the word with it's part of speech\n",
    "tokword_to_partofspeech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/kunal/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading regex_parser: Package 'regex_parser' not\n",
      "[nltk_data]     found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('regex_parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The main goal of chunking is to group words into \"noun phrases\", which is a noun with any associated verbs, adjectives, or adverbs. The part of speech tags that were generated \n",
    "in the previous step will be combined with regular expressions\n",
    "+ = match 1 or more\n",
    "? = match 0 or 1 repetitions.\n",
    "* = match 0 or MORE repetitions\t  \n",
    ". = Any character except a new line\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # combine the part-of-speech tag with a regular expression\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            # draw the chunks with nltk\n",
    "            #chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP DraperEvery/NNP time/NN)\n",
      "(Chunk Capitol/NNP dome/NN)\n",
      "(Chunk have/VBP served/VBN America/NNP)\n",
      "(Chunk Tonight/NNP)\n",
      "(Chunk Union/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk September/NNP)\n",
      "(Chunk Dictatorships/NNP shelter/NN)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Afghanistan/NNP)\n",
      "(Chunk Iraqis/NNP)\n",
      "(Chunk Lebanon/NNP)\n",
      "(Chunk Egypt/NNP)\n",
      "(Chunk Syria/NNP)\n",
      "(Chunk Burma/NNP)\n",
      "(Chunk Zimbabwe/NNP)\n",
      "(Chunk North/NNP Korea/NNP)\n",
      "(Chunk Iran/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP Draper/NNP No/NNP one/NN)\n",
      "(Chunk Islam/NNP)\n",
      "(Chunk Laden/NNP)\n",
      "(Chunk Middle/NNP East/NNP)\n",
      "(Chunk Iraq/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Beslan/NNP)\n",
      "(Chunk London/NNP)\n",
      "(Chunk Earth/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Islam/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Europe/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP Draper/NNP)\n",
      "(Chunk Afghanistan/NNP)\n",
      "(Chunk President/NNP)\n",
      "(Chunk National/NNP Assembly/NNP)\n",
      "(Chunk Iraq/NNP)\n",
      "(Chunk 're/VBP helping/VBG Iraqis/NNP)\n",
      "(Chunk Iraqi/NNP government/NN)\n",
      "(Chunk Iraqis/NNP)\n",
      "(Chunk Iraqis/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Iraq/NNP)\n",
      "(Chunk Iraqi/NNP security/NN)\n",
      "(Chunk Iraqi/NNP)\n",
      "(Chunk Fellow/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Iraqi/NNP)\n",
      "(Chunk Washington/NNP)\n",
      "(Chunk D.C/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Iraq/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Hindsight/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Iraq/NNP)\n",
      "(Chunk Iraqi/NNP)\n",
      "(Chunk Laden/NNP)\n",
      "(Chunk Zarqawi/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Laura/NNP Bush/NNP)\n",
      "(Chunk is/VBZ introduced/VBN Tuesday/NNP evening/NN)\n",
      "(Chunk Jan/NNP)\n",
      "(Chunk State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Washington/NNP)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP Draper/NNP)\n",
      "(Chunk Staff/NNP Sergeant/NNP Dan/NNP Clay/NNP)\n",
      "(Chunk Fallujah/NNP)\n",
      "(Chunk American/NNP)\n",
      "(Chunk Dan/NNP)\n",
      "(Chunk Staff/NNP Sergeant/NNP Dan/NNP Clay/NNP)\n",
      "(Chunk Lisa/NNP)\n",
      "(Chunk Sara/NNP Jo/NNP)\n",
      "(Chunk Bud/NNP)\n",
      "(Chunk Welcome/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Middle/NNP East/NNP)\n",
      "(Chunk Egypt/NNP)\n",
      "(Chunk Hamas/NNP)\n",
      "(Chunk recognize/VB Israel/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Saudi/NNP Arabia/NNP)\n",
      "(Chunk Middle/NNP East/NNP)\n",
      "(Chunk Middle/NNP East/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk House/NNP Chamber/NNP)\n",
      "(Chunk Union/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP Draper/NNP)\n",
      "(Chunk Iran/NNP)\n",
      "(Chunk Lebanon/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Tonight/NNP)\n",
      "(Chunk Iran/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Iran/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Isolationism/NNP)\n",
      "(Chunk God-given/NNP dignity/NN)\n",
      "(Chunk HIV/AIDS/NNP)\n",
      "(Chunk fight/VB AIDS/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Patriot/NNP Act/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk September/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk al/VB Qaeda/NNP)\n",
      "(Chunk Constitution/NNP)\n",
      "(Chunk Qaeda/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Qaeda/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Iraq/NNP)\n",
      "(Chunk Roosevelt/NNP)\n",
      "(Chunk Truman/NNP)\n",
      "(Chunk Kennedy/NNP)\n",
      "(Chunk Reagan/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Japan/NNP)\n",
      "(Chunk European/NNP Union/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk China/NNP)\n",
      "(Chunk India/NNP)\n",
      "(Chunk Washington/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Tonight/NNP)\n",
      "(Chunk Keeping/VBG America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk American/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Keeping/VBG America/NNP)\n",
      "(Chunk American/NNP taxpayer/NN)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Dad/NNP)\n",
      "(Chunk President/NNP Clinton/NNP)\n",
      "(Chunk Laughter/NNP)\n",
      "(Chunk Social/NNP Security/NNP)\n",
      "(Chunk Medicare/NNP)\n",
      "(Chunk Medicaid/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk save/VB Social/NNP Security/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Social/NNP Security/NNP)\n",
      "(Chunk Medicare/NNP)\n",
      "(Chunk Medicaid/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Keeping/VBG America/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk buy/VB American/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Keeping/VBG America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Keeping/VBG America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk OB/GYN/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Keeping/VBG America/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Advanced/NNP Energy/NNP Initiative/NNP)\n",
      "(Chunk Department/NNP)\n",
      "(Chunk Energy/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Middle/NNP East/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Middle/NNP Eastern/NNP oil/NN)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk keep/VB America/NNP)\n",
      "(Chunk Competitiveness/NNP Initiative/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Third/NNP)\n",
      "(Chunk No/NNP Child/NNP Left/NNP Behind/NNP Act/NNP)\n",
      "(Chunk Tonight/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Competitiveness/NNP Initiative/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Government/NNP)\n",
      "(Chunk Wise/NNP)\n",
      "(Chunk Democrat/NNP)\n",
      "(Chunk Republican/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk Justice/NNP John/NNP Roberts/NNP)\n",
      "(Chunk Justice/NNP Sam/NNP Alito/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Senate/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk American/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk Justice/NNP Sandra/NNP Day/NNP O'Connor/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Tonight/NNP)\n",
      "(Chunk Human/NNP life/NN)\n",
      "(Chunk Creator/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Washington/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Helping/NNP America/NNP)\n",
      "(Chunk Youth/NNP Initiative/NNP)\n",
      "(Chunk First/NNP Lady/NNP)\n",
      "(Chunk Laura/NNP Bush/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Gulf/NNP Coast/NNP)\n",
      "(Chunk New/NNP Orleans/NNP)\n",
      "(Chunk New/NNP Orleans/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk HIV/AIDS/NNP)\n",
      "(Chunk HIV/NNP)\n",
      "(Chunk AIDS/NNP)\n",
      "(Chunk ask/VBP Congress/NNP)\n",
      "(Chunk Ryan/NNP White/NNP Act/NNP)\n",
      "(Chunk AIDS/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk HIV/NNP)\n",
      "(Chunk AIDS/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Fellow/NNP)\n",
      "(Chunk Lincoln/NNP)\n",
      "(Chunk Martin/NNP Luther/NNP King/NNP)\n",
      "(Chunk Birmingham/NNP)\n",
      "(Chunk Selma/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk Europe/NNP)\n",
      "(Chunk May/NNP God/NNP bless/NN)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n"
     ]
    }
   ],
   "source": [
    "# We can access the chunks, which are stored as an NLTK tree \n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # combine the part-of-speech tag with a regular expression\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            for subtree in chunked.subtrees(filter=lambda t:t.label()=='Chunk'):\n",
    "                print(subtree)\n",
    "            # draw the chunks with nltk\n",
    "            #chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking with NLTK\n",
    "Sometimes there are words in the chunks that we don't won't, we can remove them using a process called chinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the chunks, which are stored as an NLTK tree \n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # The main difference here is the }{, vs. the {}. This means we're removing \n",
    "            # from the chink one or more verbs, prepositions, determiners, or the word 'to'.\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "#             for subtree in chunked.subtrees(filter=lambda t:t.label()=='Chunk'):\n",
    "#                 print(subtree)\n",
    "            # draw the chunks with nltk\n",
    "            #chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition with NLTK\n",
    "One of the most common forms of chunking in natural language processing is called \"Named Entity Recognition.\" NLTK is able to identify people, places, things, locations, monetary figures, and more.\n",
    "\n",
    "There are two major options with NLTK's named entity recognition: either recognize all named entities, or recognize named entities as their respective type, like people, places, locations, etc.\n",
    "\n",
    "Here, with the option of binary = True, this means either something is a named entity, or not. There will be no further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "#             namedEnt.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "### Text classification using NLTK\n",
    "Now that we have covered the basics of preprocessing for Natural Language Processing, we can move on to text classification using simple machine learning classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 2000\n",
      "First Review: (['plot', ':', 'a', 'separated', ',', 'glamorous', ',', 'hollywood', 'couple', 'must', 'pretend', 'to', 'reunite', 'for', 'a', 'press', 'junket', 'of', 'the', 'last', 'movie', 'that', 'they', 'ever', 'shot', 'together', '.', 'kewl', '.', '.', '.', 'now', 'i', 'only', 'wish', 'that', 'i', 'could', 'pretend', 'never', 'to', 'have', 'seen', 'this', 'movie', '.', '.', '.', 'critique', ':', 'trite', ',', 'unfunny', ',', 'boring', 'and', 'a', 'waste', 'of', 'everyone', \"'\", 's', 'talent', '.', 'how', 'a', 'premise', 'with', 'such', 'zest', 'and', 'bite', 'can', 'turn', 'into', 'a', 'movie', 'that', 'doesn', \"'\", 't', 'feature', 'any', 'chemistry', ',', 'any', 'real', 'laughs', ',', 'any', 'surprises', 'or', 'any', 'spice', 'is', 'beyond', 'me', '.', 'how', 'julia', 'roberts', 'is', 'used', 'solely', 'as', 'a', '\"', 'puppy', 'dog', '\"', 'character', ',', 'puttering', 'around', 'in', 'the', 'background', 'while', 'we', 'endure', 'the', 'complete', 'bitchiness', 'of', 'zeta', '-', 'jones', \"'\", 'character', ',', 'who', 'is', 'not', 'one', 'bit', 'funny', 'or', 'romantic', '(', 'two', 'ideal', 'ingredients', 'in', 'a', '\"', 'romantic', 'comedy', '\"', ')', ',', 'is', 'also', 'beyond', 'me', '.', 'and', 'why', 'they', 'chose', 'john', 'cusack', ',', 'a', 'great', ',', 'quirky', 'actor', 'in', 'his', 'own', 'right', ',', 'to', 'play', 'the', 'most', 'bland', ',', 'uninteresting', 'and', 'unfetching', 'character', '(', 'with', 'zero', 'chemistry', 'with', 'either', 'of', 'his', 'leads', ')', 'is', 'further', 'more', ',', 'beyond', 'me', '.', 'and', 'to', 'anybody', 'who', 'decided', 'that', 'this', 'project', 'was', '\"', 'funny', '\"', 'enough', 'to', 'greenlight', 'featuring', 'the', 'talents', 'mentioned', 'above', ',', 'along', 'with', 'billy', 'crystal', ',', 'christopher', 'walker', ',', 'seth', 'green', 'and', 'stanley', 'tucci', '.', '.', '.', 'well', ',', 'what', 'can', 'i', 'say', '.', '.', '.', 'i', 'just', 'don', \"'\", 't', 'have', 'the', 'words', '.', 'so', 'is', 'this', 'the', 'worst', 'movie', 'that', 'i', \"'\", 've', 'seen', 'all', 'year', '?', 'no', '.', 'but', 'it', 'definitely', 'sucks', 'and', 'it', \"'\", 's', 'basically', 'because', '.', '.', '.', 'well', ',', 'it', \"'\", 's', 'just', 'not', 'funny', '.', 'and', 'for', 'the', 'record', ',', 'allow', 'me', 'to', 'state', 'a', 'few', 'more', 'problems', 'with', 'it', '.', 'it', 'starts', 'off', 'slow', ',', 'it', \"'\", 's', 'got', 'no', 'energy', ',', 'it', 'doesn', \"'\", 't', 'engage', 'you', 'with', 'any', 'of', 'its', 'characters', '(', 'julia', 'barely', 'gets', 'somewhat', 'interesting', 'in', 'the', 'film', ',', 'everyone', 'else', '.', '.', '.', 'lame', '!', ')', ',', 'it', 'utilizes', 'way', 'too', 'many', 'flashbacks', 'to', 'move', 'the', 'story', 'forward', ',', 'it', \"'\", 's', 'utterly', 'predictable', ',', 'standard', ',', 'routine', ',', 'see', '-', 'through', 'and', 'uninteresting', 'as', 'a', 'plot', 'and', 'it', 'just', 'sits', 'there', 'on', 'the', 'screen', ',', 'big', 'and', 'ugly', ',', 'waiting', '.', '.', '.', 'waiting', 'for', 'you', 'to', 'laugh', 'or', 'find', 'something', 'in', 'it', 'that', 'is', 'amusing', '.', 'and', 'then', 'hank', 'azaria', 'shows', 'up', '.', '.', '.', 'aaaaaah', ',', 'the', 'film', \"'\", 's', 'savior', '(', 'mind', 'you', ',', 'some', 'might', 'be', 'offended', 'by', 'his', 'exaggeration', 'of', 'a', 'stereotype', ',', 'but', 'that', \"'\", 's', 'another', 'story', 'altogether', ')', '.', 'but', 'when', 'an', 'experienced', '\"', 'voice', '\"', 'actor', 'upstages', 'all', 'of', 'the', 'main', 'stars', 'in', 'a', 'summer', '\"', 'blockbuster', '\"', 'romantic', 'comedy', 'with', 'an', 'over', '-', 'the', '-', 'top', 'antonio', 'banderas', 'accent', ',', 'damn', 'dude', '.', '.', '.', 'your', 'film', \"'\", 's', 'in', 'trouble', '!', '!', 'rent', 'this', 'movie', 'on', 'video', 'just', 'to', 'see', 'what', 'went', 'wrong', 'yourself', '.', 'the', 'references', 'to', 'ricky', 'ricardo', 'and', 'senor', 'wences', '(', 'huh', '!', '?', ')', ',', 'the', 'idio', '-', 'plot', 'points', 'like', 'when', 'one', 'of', 'the', 'characters', 'goes', 'on', 'the', 'roof', 'to', 'stretch', 'his', 'arms', 'out', 'and', 'relax', ',', 'but', 'everyone', 'believes', 'that', 'he', \"'\", 's', 'going', 'to', 'kill', 'himself', '(', 'hardy', '-', 'har', '-', 'har', ')', 'and', 'the', 'cheap', 'way', 'of', 'getting', 'the', 'audience', 'to', 'leave', 'the', 'theater', 'laughing', 'by', 'bringing', 'back', 'a', 'ball', '-', 'sniffing', 'dog', 'that', 'has', 'no', 'place', 'being', 'in', 'the', 'location', 'at', 'the', 'end', 'of', 'the', 'movie', ',', 'well', '.', '.', '.', 'i', 'could', 'go', 'on', '.', 'but', 'i', 'won', \"'\", 't', 'because', 'i', 'do', 'still', 'respect', 'all', 'of', 'the', 'actors', 'in', 'this', 'film', 'and', 'actually', 'did', 'laugh', 'at', 'azaria', ',', 'green', 'and', 'tucci', \"'\", 's', 'antics', 'from', 'time', 'to', 'time', '(', 'ironic', ',', 'eh', '.', '.', '.', 'what', 'about', 'the', 'leads', ',', 'dammit', '!', ')', 'and', 'liked', 'the', 'premise', 'behind', 'the', 'film', '(', 'before', 'i', 'saw', 'the', 'finished', 'product', ',', 'of', 'course', ')', '.', 'a', 'dud', 'all', 'the', 'way', 'around', '.', 'btw', ',', 'all', 'the', 'talk', 'about', 'this', 'film', 'was', 'that', 'julia', 'roberts', 'was', 'to', 'be', 'in', 'a', 'fat', 'suit', 'for', 'one', 'scene', '(', 'her', 'character', 'is', 'supposed', 'to', 'have', 'lost', '60', 'pounds', ')', ',', 'so', 'when', 'the', 'scene', 'finally', 'came', ',', 'i', 'did', 'get', 'a', 'little', 'excited', 'about', 'what', 'it', 'might', 'look', 'like', 'and', 'then', '.', '.', '.', 'well', ',', 'it', 'basically', 'just', 'looked', 'like', 'julia', 'roberts', 'in', 'a', 'fat', 'suit', '!', 'ugh', '.', 'i', 'think', 'i', \"'\", 'm', 'gonna', 'start', 'drinking', 'again', 'after', 'this', 'lame', '-', 'ass', 'movie', '.', 'c', \"'\", 'mon', 'hollywood', ',', 'enough', 'with', 'the', 'crud', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'beautiful', '(', '1', '/', '10', ')', '-', 'my', 'best', 'friend', \"'\", 's', 'wedding', '(', '7', '/', '10', ')', '-', 'notting', 'hill', '(', '5', '/', '10', ')', '-', 'pretty', 'woman', '(', '7', '/', '10', ')', '-', 'runaway', 'bride', '(', '5', '/', '10', ')', '-', 'someone', 'like', 'you', '(', '4', '/', '10', ')', '-', 'wedding', 'planner', '(', '3', '/', '10', ')', '-', 'when', 'harry', 'met', 'sally', '(', '10', '/', '10', ')', '-', 'you', \"'\", 've', 'got', 'mail', '(', '4', '/', '10', ')'], 'neg')\n",
      "Most common words: [(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "The word happy: 215\n"
     ]
    }
   ],
   "source": [
    "documents=[(list(movie_reviews.words(fileid)),category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "#shuffle the documents; else will skew the results\n",
    "random.shuffle(documents)\n",
    "\n",
    "print('Number of Documents: {}'.format(len(documents)))\n",
    "print('First Review: {}'.format(documents[0]))\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words=nltk.FreqDist(all_words)\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))\n",
    "print('The word happy: {}'.format(all_words[\"happy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39768\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples']\n"
     ]
    }
   ],
   "source": [
    "word_features=list(all_words.keys())[:4000]\n",
    "print(word_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The find_features function will determine which of the 4000 word features are contained in the review\n",
    "def find_features(document):\n",
    "    words=set(document)#cuz of repeating words\n",
    "    #print(words)\n",
    "    features={}\n",
    "    for w in word_features:\n",
    "        features[w]=(w in words)\n",
    "    \n",
    "    return features\n",
    "\n",
    "features=find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "# for key,value in features.items():\n",
    "#    # prints both words,found =true/false\n",
    "#     print('{0}:{1}'.format(key,value))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do it for all the documents\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "# print(featuresets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "seed=1\n",
    "\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)\n",
    "#not X_train,X-test,ytrain ,ytesst cuz it has to do with the way they are passes to nltk through sclearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 83.0\n"
     ]
    }
   ],
   "source": [
    "# We can use sklearn algorithms in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#wrapping sckit classfire inside nltk\n",
    "model=SklearnClassifier(SVC(kernel='linear'))\n",
    "\n",
    "model.train(training)\n",
    "\n",
    "accuracy=nltk.classify.accuracy(model,testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
